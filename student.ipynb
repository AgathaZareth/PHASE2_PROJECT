{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* __Student name:__ Cassarra Groesbeck\n",
    "* __Student pace:__ Part Time/ Flex\n",
    "* __Scheduled project review date/time:__ \n",
    "* __Instructor name:__ Claude Fried\n",
    "* __Blog post URL:__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This regression modeling will yield findings to support relevant recommendations to a real estate agency, that helps homeowners buy and/or sell homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "Those findings will include:\n",
    "- a metric describing overall model performance\n",
    "- at least two regression model coefficients; that is to say, at least two feature-specific effects on Sale Price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "The real estate agency needs to provide advice to homeowners about how home renovations might increase the estimated value of their homes, and by what amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "This project uses the King County House Sales dataset. For more information, other than what is provided below, see the [King County Assessor Website](https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r)\n",
    "\n",
    "###  Column Names and Descriptions for King County Data Set\n",
    "\n",
    "\n",
    "| Column     | Description   |\n",
    "|------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|  `id`         | **Unique identifier for a house**  |\n",
    "| `date`        | **Date house was sold**  |\n",
    "| `price`       | **Sale price (prediction target)** |\n",
    "| `bedrooms`    | **Number of bedrooms**  |\n",
    "|`bathrooms`    | **Number of bathrooms**   |\n",
    "|`sqft_living`  | **Square footage of living space in the home**  |\n",
    "| `sqft_lot`    | **Square footage of the lot**   |\n",
    "|  `floors`     | **Number of floors (levels) in house**  |\n",
    "| `waterfront`  | **Whether the house is on a waterfront**  |\n",
    "| `view`        | **Quality of view from house** |\n",
    "| `condition`   | **How good the overall condition of the house is. Related to maintenance of house.**  |\n",
    "| `grade`       | **Overall grade of the house. Related to the construction and design of the house.**  |\n",
    "| `sqft_above`  | **Square footage of house apart from basement**  |\n",
    "|`sqft_basement`| **Square footage of the basement**   |\n",
    "|  `yr_built`   | **Year when house was built**  |\n",
    "| `yr_renovated`| **Year when house was renovated**  |\n",
    "| `zipcode`     | **ZIP Code used by the United States Postal Service** |\n",
    "| `lat`         | **Latitude coordinate**  |\n",
    "| `long`        | **Longitude coordinate**   |\n",
    "|`sqft_living15`| **The square footage of interior housing living space for the nearest 15 neighbors**   |\n",
    "| `sqft_lot15`  | **The square footage of the land lots of the nearest 15 neighbors**   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# statsmodels\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "# rando\n",
    "from itertools import combinations\n",
    "\n",
    "#visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/kc_house_data.csv\n",
    "data = pd.read_csv(\"data/kc_house_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return full statsmodel summary\n",
    "def model_it_full(df, target):\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    \n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return r_squared values, p table from .summary, and a \n",
    "# couple of residual normality checks (hist and qq plot)\n",
    "\n",
    "def model_it_small(df, target):\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "    #statsmodel fit\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()  \n",
    "    \n",
    "    #kfold\n",
    "    regression = LinearRegression()\n",
    "    crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    kfold_r = np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "    \n",
    "    #PLOTS\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    fig.suptitle('Normality of Residuals')\n",
    "    #hist\n",
    "    sns.histplot(model.resid, ax=ax0)\n",
    "    ax0.set(xlabel='Residual', ylabel='Frequency', title='Distribution of Residuals')\n",
    "    #qq\n",
    "    sm.qqplot(model.resid, fit = True, line = '45', ax=ax1)\n",
    "    ax1.set(title='QQ Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    #print r_squared values\n",
    "    print(f'r_sq: {model.rsquared}. r_sq_adjusted: {model.rsquared_adj}. k_fold_r: {kfold_r}')\n",
    "    \n",
    "    #return \n",
    "    return model.summary().tables[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make colinearity check function\n",
    "# code from Multicollinearity of Features - Lab, turned it into a function\n",
    "\n",
    "def colinearity(df):\n",
    "    #get absolute value of correlations, sort them, and turn into new DF called df\n",
    "    df=df.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "    # zip the columns (Which were only named level_0 and level_1 by default) \n",
    "    # into a new column named \"pairs\"\n",
    "    df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "    # set index to pairs\n",
    "    df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "    # drop level_ columns\n",
    "    df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "    # rename correlation column as cc rather than 0\n",
    "    df.columns = ['cc']\n",
    "\n",
    "    # just correlations over .75, but less than 1.\n",
    "    df = df[(df.cc>.75) & (df.cc <1)]\n",
    "\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colinearity with VIF\n",
    "# code from Linear Regression - Cumulative Lab, altered to make a df w/sorted values\n",
    "\n",
    "def get_VIFs_above5(df, target):\n",
    "\n",
    "    vif_data = sm.add_constant(df.drop(target, axis=1))\n",
    "\n",
    "    vif = [variance_inflation_factor(vif_data.dropna().values, i)\\\n",
    "           for i in range(vif_data.dropna().shape[1])]\n",
    "\n",
    "    vif_df = pd.DataFrame(vif, index=vif_data.columns).sort_values(0, ascending=False)\n",
    "    return vif_df[vif_df[0]>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_from_pdDataFrame(df):\n",
    "    return df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "\n",
    "def remove_outliers_from_Column(df, column):\n",
    "    return df[(np.abs(stats.zscore(df[column])) < 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Catagorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = data.select_dtypes(include=object)\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore catagorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] 'date' (will become ordinal)\n",
    "- [x] 'waterfront' (will become boolean)\n",
    "- [x] 'view' (stays catagorical)\n",
    "- [x] 'condtion' (stays catagorical)\n",
    "- [x] 'grade' (ordinal)\n",
    "- [x] 'sqft_basement' (continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from obj_df exploration:\n",
    "- 'waterfront'\n",
    " - has two values: NO & YES\n",
    " - has 2376 nulls, that will need to be addressed before ohe'ing\n",
    " - 11% of data is null/ missing\n",
    " - 0.7% of properties are waterfront\n",
    " - 88.3% of properites are not on waterfront\n",
    " - I will change nulls to NO due to less than 1% of properties on waterfront\n",
    "- 'condition'\n",
    " - 5 unique values\n",
    " - has zero nulls\n",
    "- 'view'\n",
    " - has 6 values \n",
    " - 89.93% is 'NONE'\n",
    " - 63 nulls (0.29%), change to 'NONE' \n",
    "- 'grade'\n",
    " - 11 unique values\n",
    " - has numeric value (3-13) and word description (ex \"poor\" or \"good\") associated with each grade assignment\n",
    " - need to change to just number grade and delete description\n",
    "- 'date' \n",
    " - string: 'mm/dd/yyyy'\n",
    "- 'sqft_basement'\n",
    " - float values cast as string\n",
    " - 454 missing, shown as '?', 2% missing\n",
    " - 12826 '0.0' basement, ie 59% no basement, add new column \"has_basement\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__ for obj_df features to be ohe-ready. I will add to this list as I explore data and will address needed conversions at end before ohe'ing.\n",
    "\n",
    "1. [x] replace 'waterfront' {np.nan:'NO'} - this will be a boolean feature\n",
    "2. [x] change 'view' nulls to 'NONE' - will stay catagorical\n",
    "3. [x] keep 'grade' number and ditch description - this will make the feature ordinal\n",
    "4. [x] convert 'date' to just numerical month - ordinal\n",
    "5. [x] for 'sqft_basement' make new column \"has_basement\"\n",
    "6. [x] if value '0.0' or '?' append new column 0, else 1 - this will be a boolean feature\n",
    "7. [x] make new get_dummies_df of ['view', 'condition', 'has_basement', 'waterfront']\n",
    "8. [x] pd.get_dummies(dummies_df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print .value_counts() for each column in obj_df\n",
    "for column in obj_df.columns:\n",
    "    print(f\"COLUMN: '{column}'\")\n",
    "    print(f\"Number of unique values: {len(obj_df[column].unique())}\")\n",
    "    print(f\"Number of nulls: {obj_df[column].isnull().sum()}\")\n",
    "    print(obj_df[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  null counts\n",
    "obj_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[['sqft_basement']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obj_df['date'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tackeling TODO ohe prep list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. replace 'waterfront' {np.nan:'NO'}\n",
    "data['waterfront'].replace({np.nan:'NO'}, inplace=True)\n",
    "\n",
    "#check\n",
    "data['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. change 'view' nulls to 'NONE'\n",
    "data['view'].fillna('NONE', inplace=True)\n",
    "\n",
    "#check\n",
    "data['view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. keep 'grade' number (as an int) and ditch description\n",
    "data['grade'] = [int(grade[:2]) for grade in data['grade']]\n",
    "\n",
    "#check\n",
    "data['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. convert 'date' to just numerical month\n",
    "data['date'] = pd.DatetimeIndex(data['date']).month\n",
    "\n",
    "#check\n",
    "data['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. & 6. for 'sqft_basement' make new column \"has_basement\"\n",
    "# if value '0.0' or '?' append new column 0, else 1\n",
    "\n",
    "basement = []\n",
    "for square_feet in data['sqft_basement']:\n",
    "    if square_feet == '0.0':\n",
    "        basement.append('NO')\n",
    "    elif square_feet == '?':\n",
    "        basement.append('NO')\n",
    "    else:\n",
    "        basement.append('YES')\n",
    "        \n",
    "data['has_basement'] = basement\n",
    "\n",
    "#drop 'TotalBsmtSF'\n",
    "data = data.drop('sqft_basement', axis=1)\n",
    "\n",
    "# check \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. make new dummies_df of ['view', 'condition']\n",
    "dummies_df = data[['view','condition', 'has_basement', 'waterfront']]\n",
    "dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the values to note which feature has been dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. pd.get_dummies(dummies_df, drop_first=True)\n",
    "dummies_df = pd.get_dummies(dummies_df, drop_first=True)\n",
    "dummies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __NOTES:__\n",
    "Take note of the features that were dropped:\n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract out columns with Dtype == int or float for further exploration\n",
    "cont_df = data.select_dtypes(exclude=object).drop(['id', 'price'], axis=1)\n",
    "cont_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls \n",
    "cont_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __NOTES:__ \n",
    "3,842 missing values from 'yr_renovated'. Thats too many for imputations or replacement. Nulls may mean N/A. Could turn into boolean: 'renovated_YES' == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __NOTES:__ \n",
    "An additional 17,011 values of 0, ie 0 likely means N/A and missing values are just that, missing. That is in fact too many for imputations or replacement. I will need to drop this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting zip code to cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zipcode'].value_counts() # ohe these, find way to reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping for City Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = requests.get('https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?FIPS=53033') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.prettify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract out just zipcode and city from https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?FIPS=53033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab an easy to identify thing\n",
    "span = soup.find('span')\n",
    "\n",
    "# move up to find sibling of container I want\n",
    "parent = span.parent\n",
    "\n",
    "# get to the correct container\n",
    "box = parent.next_sibling.next_sibling\n",
    "\n",
    "# get text from container and format how needed\n",
    "box_text = box.get_text().replace('\\n', ',')\n",
    "box_text = box_text.replace('Zip CodeCityCounty', '')\n",
    "box_text = box_text.replace('King County,', '')\n",
    "box_text = box_text.replace(' ... ', '')\n",
    "\n",
    "# split on the commas and remove last (empty) element\n",
    "lst = box_text.split(\",\")\n",
    "lst.pop()\n",
    "\n",
    "#check\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate into zipcodes and cities\n",
    "codes = []\n",
    "cities = []\n",
    "i=0\n",
    "for element in lst:\n",
    "    if i %2 == 0:\n",
    "        codes.append(element)\n",
    "        i+=1\n",
    "    else:\n",
    "        cities.append(element)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a DF with _code_ and _cities_ lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the two lists to make a DF\n",
    "# empty df\n",
    "web_df = pd.DataFrame()\n",
    "web_df['zipcode_web']  = codes\n",
    "web_df['city_web']  = cities\n",
    "\n",
    "#check\n",
    "web_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use DF to make a dict of \n",
    "dictionary = {}\n",
    "for key in web_df['zipcode_web'].unique():\n",
    "      dictionary[key] = str(web_df[web_df['zipcode_web'] == key]['city_web'].unique())\n",
    "        \n",
    "# check\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new column on exisiting _data_ df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['zipcode'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy zipcode to new column\n",
    "data['Location/Area'] = data['zipcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dictionary to replace zipcodes with cities\n",
    "data['Location/Area'] = data['Location/Area'].replace(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distill Cities down to Areas__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new column (now containing city names) to begin a list of areas\n",
    "new_values = []\n",
    "for cities in data['Location/Area']:\n",
    "    if 'Seattle' in cities:\n",
    "        new_values.append('Seattle Area')\n",
    "    elif 'Bellevue' in cities:\n",
    "        new_values.append('Bellevue Area')\n",
    "    elif 'Auburn' in cities:\n",
    "        new_values.append('Auburn Area')\n",
    "    elif 'Kent' in cities:\n",
    "        new_values.append('Kent Area')\n",
    "    else:\n",
    "        new_values.append(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column values from cities to Areas (when possible) otherwise remains city name\n",
    "data['Location/Area'] = new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data['Location/Area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__\n",
    "- [x] '98077' needs a City name: Woodinville\n",
    "- [x] if two cites distilled need to assign an area\n",
    " - [x] Bothell area: Kenmore, _Bothell_\n",
    " - [x] Bellevue Area: _Kirkland_\n",
    " - [x] Sammamish Area: _Sammamish_, _Issaquah_, _Redmond_\n",
    " - [x] Newcastle area: Newcastle, _Renton_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat above, further distilling values from 'Location/Area' column\n",
    "new_values2 = []\n",
    "for cities in data['Location/Area']:\n",
    "    if 'Bothell' in cities:\n",
    "        new_values2.append('Bothell Area')\n",
    "    elif 'Kirkland' in cities:\n",
    "        new_values2.append('Bellevue Area')\n",
    "    elif 'Renton' in cities:\n",
    "        new_values2.append('Newcastle Area')\n",
    "    elif 'Sammamish' in cities:\n",
    "        new_values2.append('Sammamish Area')\n",
    "    elif 'Issaquah' in cities:\n",
    "        new_values2.append('Sammamish Area')\n",
    "    elif 'Redmond' in cities:\n",
    "        new_values2.append('Sammamish Area')\n",
    "    elif cities == '98077':\n",
    "        new_values2.append('Woodinville')\n",
    "    else:\n",
    "        new_values2.append(cities.strip(\"['']\"))\n",
    "\n",
    "# change column values to new list\n",
    "data['Location/Area'] = new_values2\n",
    "\n",
    "# check\n",
    "data['Location/Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good enough for get_dummies then add to dummies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies for new column $data['Location/Area']$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_dummies = pd.get_dummies(data[['Location/Area']], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate with existing dummies_df\n",
    "dummies_df = pd.concat([dummies_df, zipcode_dummies], axis=1)\n",
    "\n",
    "# check\n",
    "dummies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paring Down Data:\n",
    "__Get two df's in order__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up dummies_df (catagorical features)\n",
    "\n",
    "# move boolean columns to dummies df\n",
    "dummies_df['waterfront_YES'] = data['waterfront']\n",
    "dummies_df['has_basement_YES'] = data['has_basement']\n",
    "\n",
    "# check\n",
    "dummies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cont_df (continuous features)\n",
    "\n",
    "# re define cont_df with relevant columns\n",
    "# leave 'price' for now\n",
    "cont_df = data.drop(['id', 'yr_renovated', 'view', 'condition', 'Location/Area', 'lat', 'long', 'zipcode', 'waterfront', 'has_basement'], axis=1)\n",
    "\n",
    "#check\n",
    "cont_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check continuous features, some maybe catagorical features let in, they are ordinal\n",
    "\n",
    "#looping over all columns  \n",
    "plots = cont_df.drop('price', axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(12, 15))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(plots.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    sns.regplot(x = col, y = 'price', data = cont_df, ax=ax, line_kws={\"color\": \"tab:red\"})\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few almost a flat lines, ie zero relationship\n",
    "# drop \n",
    "cont_df = cont_df.drop(['floors', 'yr_built', 'date', 'sqft_lot15', 'sqft_lot'], axis=1)\n",
    "\n",
    "#check\n",
    "cont_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there really a 30+ bedroom house?\n",
    "cont_df['bedrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop that 1 it's obviously an anomaly \n",
    "cont_df = cont_df[cont_df['bedrooms']<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again now that features have been dropped\n",
    "plots = cont_df.drop('price', axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(12, 10))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(plots.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    sns.regplot(x = col, y = 'price', data = cont_df, ax=ax, line_kws={\"color\": \"tab:red\"})\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build baseline model with highest correlated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df.corr()['price'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'sqft_living' as baseline model feature\n",
    "# baseline model\n",
    "baseline_model_df = cont_df[['sqft_living', 'price']]\n",
    "y = baseline_model_df[target]\n",
    "X = baseline_model_df.drop(target, axis=1)\n",
    "\n",
    "model_1 = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the residuals\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.histplot(model_1.resid, ax=ax0)\n",
    "ax0.set(xlabel='Residual', ylabel='Frequency', title='Distribution of Residuals')\n",
    "\n",
    "sm.qqplot(model_1.resid, fit = True, line = '45', ax=ax1)\n",
    "ax1.set(title='QQ Plot')\n",
    "\n",
    "fig.suptitle('Normality of Residuals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oofs, not great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make functions for further modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Multiple R_squared Values (to get an idea of where to start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unedited raw data\n",
    "full_dfs = pd.concat([cont_df, dummies_df], axis=1).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log and Scale \n",
    "# log transform\n",
    "log_df = np.log(cont_df)\n",
    "\n",
    "# standardized AND logged\n",
    "log_stand_df = log_df.apply(standardize)\n",
    "\n",
    "# standardize ONLY\n",
    "stand_df = cont_df.apply(standardize)\n",
    "\n",
    "# concat with dummies\n",
    "model_log = pd.concat([log_df, dummies_df], axis=1).dropna(axis=0)             #logged only\n",
    "model_stand = pd.concat([stand_df, dummies_df], axis=1).dropna(axis=0)         #scaled only \n",
    "model_log_stand = pd.concat([log_stand_df, dummies_df], axis=1).dropna(axis=0) #logged and scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from everything\n",
    "filtered_df = remove_outliers_from_pdDataFrame(cont_df)\n",
    "\n",
    "# concat with dummies\n",
    "model_filtered = pd.concat([filtered_df, dummies_df], axis=1).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features relative to remodels\n",
    "#list(cont_df.columns)\n",
    "\n",
    "reno_features = ['price',\n",
    " 'bedrooms',\n",
    " 'bathrooms',\n",
    " 'sqft_living',\n",
    " 'grade',\n",
    " 'condition_Fair',\n",
    " 'condition_Good',\n",
    " 'condition_Poor',\n",
    " 'condition_Very Good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier removed and logged\n",
    "outliers_log = np.log(filtered_df)\n",
    "\n",
    "# concat with dummies\n",
    "model_outl_fltd = pd.concat([outliers_log, dummies_df], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test multiple df's quickly before moving on to removing colinear features\n",
    "\n",
    "dfs = [baseline_model_df,               #1. baseline, 'sqft_living' only\n",
    "         cont_df,                       #2. continuous features only\n",
    "         full_dfs,                      #3. cont and dummies\n",
    "         model_log,                     #4. all cont features logged\n",
    "         model_stand,                   #5. all cont features scaled\n",
    "         model_log_stand,               #6. all cont features logged and scaled\n",
    "         model_filtered,                #7. all cont features outliers removed\n",
    "         model_outl_fltd,               #8. cont outliers removed and logged\n",
    "         full_dfs[reno_features]]       #9. reno specific features \n",
    "\n",
    "n=0\n",
    "for df in dfs:\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "    print(f'{n}. {model.rsquared}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comparisons of continuous data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "columnz = list(cont_df.columns)\n",
    "colorz = ['red', 'purple', 'blue', 'green', 'yellow', 'orange', 'cyan']\n",
    "\n",
    "i=0\n",
    "for i in range(len(columnz)):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    #fig.suptitle(f'{columnz[i]}')\n",
    "    \n",
    "    sns.histplot(ax=axes[0], data=cont_df[columnz[i]], bins='auto', color=colorz[0], alpha=.7)\\\n",
    "    .set(title=\"RAW\")\n",
    "    \n",
    "    sns.histplot(ax=axes[1], data=filtered_df[columnz[i]], bins='auto', color=colorz[1], alpha=.7)\\\n",
    "    .set(title=\"OUTLIERS REMOVED\")\n",
    "    \n",
    "    sns.histplot(ax=axes[2], data=log_df[columnz[i]], bins='auto', color=colorz[2], alpha=.7)\\\n",
    "    .set(title=\"LOG TRANSFORMED\")\n",
    "    \n",
    "    sns.histplot(ax=axes[3], data=stand_df[columnz[i]], bins='auto', color=colorz[3], alpha=.7)\\\n",
    "    .set(title=\"STANDARDIZED\")\n",
    "    \n",
    "    sns.histplot(ax=axes[4], data=log_stand_df[columnz[i]], bins='auto', color=colorz[4], alpha=.7)\\\n",
    "    .set(title=\"LOG & STAND\")\n",
    "    \n",
    "    plt.show();\n",
    "    i+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTES:__\n",
    "\n",
    "DF 'model_log' has highest r_squared score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 'model_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_it_small(model_log, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove 'view_FAIR' (p_value: 0.909) and remodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_drop_1p = model_log.drop('view_FAIR', axis=1)\n",
    "model_it_small(log_drop_1p, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity of Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colinearity(log_drop_1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_VIFs_above5(log_drop_1p, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop 'sqft_above' and remodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_colin_feature = log_drop_1p.drop('sqft_above', axis=1).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_it_small(drop_colin_feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __NOTES:__\n",
    "'has_basement_YES' has p_values above .05, and CI spans 0. Remove this feature and remodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = drop_colin_feature[target]\n",
    "X = drop_colin_feature.drop(target, axis=1)\n",
    "\n",
    "#statsmodel fit\n",
    "model = sm.OLS(y, sm.add_constant(X)).fit()  \n",
    "\n",
    "# expo it, data has been loggend?\n",
    "np.exp(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = drop_colin_feature.drop(target, axis=1)\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "features = list(X.columns)\n",
    "combos = combinations(features, 2)\n",
    "r_2s_dict = {pair:None for pair in combos}\n",
    "\n",
    "# use pairs list and find r_2's\n",
    "for k,v in r_2s_dict.items():\n",
    "    \n",
    "    # make copy of df so you don't mess anything up\n",
    "    X_interact = X.copy()\n",
    "    \n",
    "    # use pairs\n",
    "    # new column in X_interact with product of predictors\n",
    "    X_interact[f'{k}'] = X[f'{k[0]}'] * X[f'{k[1]}']\n",
    "    # r2 with combo feature added\n",
    "    r_2_with_interaction = np.mean(cross_val_score(regression, X_interact, y, scoring='r2', cv=crossvalidation))\n",
    "    # store r_2 and pair in a dictionary\n",
    "    r_2s_dict[k] = r_2_with_interaction\n",
    "    \n",
    "\n",
    "# sort by r_2 value and extract top 3 pairs (the last 3)\n",
    "top_5 = dict(sorted(r_2s_dict.items(), key=lambda item: item[1])[-5:])\n",
    "\n",
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get r_squared with interactions added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine X so this cell can run multiple times\n",
    "X = drop_colin_feature.drop(target, axis=1)\n",
    "\n",
    "# add interactions columns to df \n",
    "# TOP 3\n",
    "X[\"'grade'*'Location/Area_Seattle area'\"] = X['grade'] * X['Location/Area_Seattle area']\n",
    "X[\"'bathrooms'*'sqft_living'\"] = X['bathrooms'] * X['sqft_living']\n",
    "X[\"'sqft_living'*'grade'\"] = X['grade'] * X['sqft_living']\n",
    "\n",
    "# TOP 5\n",
    "X[\"'bathrooms'*'grade'\"] = X['bathrooms']*X['grade']\n",
    "X[\"'bedrooms'*'grade'\"]= X['bedrooms']*X['grade']\n",
    "\n",
    "# Then use 10-fold cross-validation ...\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "#TOP 3: 0.7748541615156376\n",
    "#TOP 5: 0.7748750703784515\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[target] = drop_colin_feature[target]\n",
    "model_it_small(X, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity?\n",
    "plots = seattle_reno_change.drop(target, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(10, 3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(plots.columns):\n",
    "    ax = axes[index]\n",
    "    sns.regplot(x = col, y = 'price', data = cont_df, ax=ax, line_kws={\"color\": \"tab:red\"})\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals against predicted values to \n",
    "y = seattle_reno_change[target]\n",
    "X2 = seattle_reno_change.drop(target, axis=1)\n",
    "\n",
    "#statsmodel fit\n",
    "model = sm.OLS(y, sm.add_constant(X2)).fit()\n",
    "y_pred = model.fittedvalues\n",
    "\n",
    "# check for homoscedasticity\n",
    "p = sns.scatterplot(x=y_pred,y=model.resid)\n",
    "plt.xlabel('Predicted y values')\n",
    "plt.ylabel('Residuals')\n",
    "#plt.xlim(70,100)\n",
    "p = sns.lineplot(x=[y_pred.min(),y_pred.max()],y=[0,0],color='blue')\n",
    "p = plt.title('Residuals vs Predicted y value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Multicollinearity (Independence Assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colinearity(seattle_reno_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_VIFs_above5(seattle_reno_change, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove 'sqft_living'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_it_small(seattle_reno_change.drop('sqft_living', axis=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_VIFs_above5(seattle_reno_change.drop('sqft_living', axis=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expoed = np.exp(seattle_reno_change)\n",
    "y = expoed[target]\n",
    "X2 = expoed.drop(target, axis=1)\n",
    "\n",
    "#statsmodel fit\n",
    "model = sm.OLS(y, sm.add_constant(X2)).fit() \n",
    "\n",
    "# expo it, data has been loggend?\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO only log target, that will make coeffs in percentages, \n",
    "# find the dropped dummies values\n",
    "# maybe remove outliers? prob not\n",
    "# work on notebook flow\n",
    "# need big header FINAL MODEL\n",
    "# final model checks\n",
    "# header for interpretations\n",
    "# keep floors\n",
    "#use sqft_living15 and subtract "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
